{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol, List, Tuple, Callable, Dict, Type, Optional\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import heapq\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "\n",
    "SetType = List[Tuple]\n",
    "\n",
    "def flatten_sets(nested_sets):\n",
    "    return set(obj for group in nested_sets for obj in group)\n",
    "\n",
    "def safe_deepcopy(sets: SetType) -> SetType:\n",
    "    return copy.deepcopy(sets)\n",
    "\n",
    "\n",
    "def swap_elements(sets: SetType, i: int, j: int, a_idx: int, b_idx: int) -> None:\n",
    "    sets[i][a_idx], sets[j][b_idx] = sets[j][b_idx], sets[i][a_idx]\n",
    "\n",
    "\n",
    "\n",
    "class Optimizer(Protocol):\n",
    "    def __init__(self, sets: List[SetType], objective_function: Callable, params: Dict):\n",
    "        pass\n",
    "    \n",
    "    def compute(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def optimize(self) -> None:\n",
    "        pass\n",
    "\n",
    "def validate_sets(original_objects: List[SetType], final_sets: List[SetType]) -> bool:\n",
    "    def count_elements(objects):\n",
    "        counts = {}\n",
    "        for obj in objects:\n",
    "            key = tuple(obj)  \n",
    "            counts[key] = counts.get(key, 0) + 1\n",
    "        return counts\n",
    "    \n",
    "    original_counts = count_elements(original_objects)\n",
    "    final_counts = count_elements([obj for s in final_sets for obj in s])\n",
    "    \n",
    "    return original_counts == final_counts\n",
    "\n",
    "class GreedyOptimizer:\n",
    "    def __init__(self, objects: SetType, m: int, objective_function: Callable, params: Dict = {}):\n",
    "        self.objects = sorted(objects, key=lambda x: -x[1])\n",
    "        self.sets = [[] for _ in range(m)]\n",
    "        self.objective_function = objective_function\n",
    "        self.score = float('inf')\n",
    "\n",
    "    def compute(self) -> None:\n",
    "        self.score = self.objective_function(self.sets)\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        heap = [(0, i) for i in range(len(self.sets))]\n",
    "        heapq.heapify(heap)\n",
    "\n",
    "        for obj in self.objects:\n",
    "            size, min_index = heapq.heappop(heap)\n",
    "            self.sets[min_index].append(obj)\n",
    "            heapq.heappush(heap, (size + obj[1], min_index))\n",
    "\n",
    "        self.compute()\n",
    "\n",
    "class SwapOptimizer:\n",
    "    def __init__(self, sets: List[SetType], objective_function: Callable, params: Dict):\n",
    "        self.sets = sets\n",
    "        self.objective_function = objective_function\n",
    "        self.max_iterations = params.get(\"max_iterations\", 1000)\n",
    "        self.score = float('inf')\n",
    "\n",
    "    def compute(self) -> None:\n",
    "        self.score = self.objective_function(self.sets)\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        self.compute()  # compute initial score\n",
    "        for _ in range(self.max_iterations):\n",
    "            non_empty_sets = [i for i, s in enumerate(self.sets) if len(s) > 1]\n",
    "            if len(non_empty_sets) < 2:\n",
    "                break\n",
    "\n",
    "            i, j = random.sample(non_empty_sets, 2)\n",
    "            obj_i_idx, obj_j_idx = random.randrange(len(self.sets[i])), random.randrange(len(self.sets[j]))\n",
    "            obj_i, obj_j = self.sets[i][obj_i_idx], self.sets[j][obj_j_idx]\n",
    "            \n",
    "            if obj_i == obj_j:\n",
    "                continue\n",
    "\n",
    "            # Perform swap\n",
    "            self.sets[i][obj_i_idx], self.sets[j][obj_j_idx] = obj_j, obj_i\n",
    "\n",
    "            # Evaluate new score\n",
    "            new_score = self.objective_function(self.sets)\n",
    "\n",
    "            if new_score < self.score:\n",
    "                self.score = new_score\n",
    "            else:\n",
    "                # Revert swap\n",
    "                self.sets[i][obj_i_idx], self.sets[j][obj_j_idx] = obj_i, obj_j\n",
    "\n",
    "\n",
    "class SimulatedAnnealingOptimizer:\n",
    "    def __init__(self, sets: List[SetType], objective_function: Callable, params: Dict):\n",
    "        self.sets = sets\n",
    "        self.objective_function = objective_function\n",
    "        self.max_iterations = params.get(\"max_iterations\", 1000)\n",
    "        self.temperature = params.get(\"initial_temp\", 10.0)\n",
    "        self.cooling_rate = params.get(\"cooling_rate\", 0.995)\n",
    "        self.score = float(\"inf\")\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        # Avalia score inicial\n",
    "        current_score = self.objective_function(self.sets)\n",
    "        best_sets = [list(s) for s in self.sets]\n",
    "        best_score = current_score\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            if self.temperature < 1e-6:\n",
    "                break\n",
    "\n",
    "            non_empty_sets = [i for i, s in enumerate(self.sets) if len(s) > 1]\n",
    "            if len(non_empty_sets) < 2:\n",
    "                break\n",
    "\n",
    "            i, j = random.sample(non_empty_sets, 2)\n",
    "            obj_i_idx, obj_j_idx = random.randrange(len(self.sets[i])), random.randrange(len(self.sets[j]))\n",
    "\n",
    "            obj_i, obj_j = self.sets[i][obj_i_idx], self.sets[j][obj_j_idx]\n",
    "            if obj_i == obj_j:\n",
    "                continue\n",
    "\n",
    "            # Swap\n",
    "            self.sets[i][obj_i_idx], self.sets[j][obj_j_idx] = obj_j, obj_i\n",
    "            new_score = self.objective_function(self.sets)\n",
    "            delta = current_score - new_score\n",
    "\n",
    "            # Critério de aceitação de Simulated Annealing\n",
    "            if new_score < current_score or np.exp(delta / self.temperature) > random.random():\n",
    "                current_score = new_score\n",
    "                if new_score < best_score:\n",
    "                    best_score = new_score\n",
    "                    best_sets = [list(s) for s in self.sets]\n",
    "            else:\n",
    "                # Reverte swap\n",
    "                self.sets[i][obj_i_idx], self.sets[j][obj_j_idx] = obj_i, obj_j\n",
    "\n",
    "            self.temperature *= self.cooling_rate\n",
    "\n",
    "        self.sets = best_sets\n",
    "        self.score = best_score\n",
    "\n",
    "class KernighanLinOptimizer:\n",
    "    def __init__(self, sets: SetType, objective_function: Callable, params: Dict):\n",
    "        self.sets = sets\n",
    "        self.objective_function = objective_function\n",
    "        self.params = params\n",
    "        self.score = float(\"inf\")\n",
    "\n",
    "    def compute(self):\n",
    "        self.score = self.objective_function(self.sets)\n",
    "\n",
    "    def optimize(self):\n",
    "        improved = True\n",
    "        while improved:\n",
    "            improved = False\n",
    "            best_gain = 0\n",
    "            best_move = None\n",
    "            for i in range(len(self.sets)):\n",
    "                for j in range(i + 1, len(self.sets)):\n",
    "                    for a_idx, a in enumerate(self.sets[i]):\n",
    "                        for b_idx, b in enumerate(self.sets[j]):\n",
    "                            new_sets = safe_deepcopy(self.sets)\n",
    "                            swap_elements(new_sets, i, j, a_idx, b_idx)\n",
    "                            new_score = self.objective_function(new_sets)\n",
    "                            gain = self.score - new_score\n",
    "                            if gain > best_gain:\n",
    "                                best_gain = gain\n",
    "                                best_move = (i, j, a_idx, b_idx)\n",
    "            if best_move:\n",
    "                i, j, a_idx, b_idx = best_move\n",
    "                swap_elements(self.sets, i, j, a_idx, b_idx)\n",
    "                self.score -= best_gain\n",
    "                improved = True\n",
    "\n",
    "\n",
    "class MigrationOptimizer:\n",
    "    def __init__(self, sets: SetType, objective_function: Callable, params: Dict):\n",
    "        self.sets = sets\n",
    "        self.objective_function = objective_function\n",
    "        self.params = params\n",
    "        self.score = float(\"inf\")\n",
    "\n",
    "    def compute(self):\n",
    "        self.score = self.objective_function(self.sets)\n",
    "\n",
    "    def optimize(self):\n",
    "        improved = True\n",
    "        while improved:\n",
    "            improved = False\n",
    "            best_gain = 0\n",
    "            best_move = None\n",
    "            for i in range(len(self.sets)):\n",
    "                for j in range(len(self.sets)):\n",
    "                    if i == j or not self.sets[i]:\n",
    "                        continue\n",
    "                    for idx, obj in enumerate(self.sets[i]):\n",
    "                        new_sets = safe_deepcopy(self.sets)\n",
    "                        obj_moved = new_sets[i].pop(idx)\n",
    "                        new_sets[j].append(obj_moved)\n",
    "                        new_score = self.objective_function(new_sets)\n",
    "                        gain = self.score - new_score\n",
    "                        if gain > best_gain:\n",
    "                            best_gain = gain\n",
    "                            best_move = (i, j, idx)\n",
    "            if best_move:\n",
    "                i, j, idx = best_move\n",
    "                obj = self.sets[i].pop(idx)\n",
    "                self.sets[j].append(obj)\n",
    "                self.score -= best_gain\n",
    "                improved = True\n",
    "\n",
    "\n",
    "class TabuSearchOptimizer:\n",
    "    def __init__(self, sets: SetType, objective_function: Callable, params: Dict):\n",
    "        self.sets = sets\n",
    "        self.objective_function = objective_function\n",
    "        self.tabu_size = params.get(\"tabu_size\", 50)\n",
    "        self.iterations = params.get(\"max_iterations\", 100)\n",
    "        self.score = float(\"inf\")\n",
    "\n",
    "    def compute(self):\n",
    "        self.score = self.objective_function(self.sets)\n",
    "\n",
    "    def optimize(self):\n",
    "        tabu_list = []\n",
    "        best_score = self.score\n",
    "        best_solution = safe_deepcopy(self.sets)\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            best_gain = 0\n",
    "            best_move = None\n",
    "            for i in range(len(self.sets)):\n",
    "                for j in range(i + 1, len(self.sets)):\n",
    "                    for a_idx, a in enumerate(self.sets[i]):\n",
    "                        for b_idx, b in enumerate(self.sets[j]):\n",
    "                            move = ((i, a_idx), (j, b_idx))\n",
    "                            if move in tabu_list:\n",
    "                                continue\n",
    "                            new_sets = safe_deepcopy(self.sets)\n",
    "                            swap_elements(new_sets, i, j, a_idx, b_idx)\n",
    "                            new_score = self.objective_function(new_sets)\n",
    "                            gain = self.score - new_score\n",
    "                            if gain > best_gain:\n",
    "                                best_gain = gain\n",
    "                                best_move = move\n",
    "            if best_move:\n",
    "                (i, a_idx), (j, b_idx) = best_move\n",
    "                swap_elements(self.sets, i, j, a_idx, b_idx)\n",
    "                self.score -= best_gain\n",
    "                tabu_list.append(best_move)\n",
    "                if len(tabu_list) > self.tabu_size:\n",
    "                    tabu_list.pop(0)\n",
    "                if self.score < best_score:\n",
    "                    best_score = self.score\n",
    "                    best_solution = safe_deepcopy(self.sets)\n",
    "        self.sets = best_solution\n",
    "\n",
    "\n",
    "class GeneticOptimizer:\n",
    "    def __init__(self, sets: SetType, objective_function: Callable, params: Dict):\n",
    "        self.sets = sets\n",
    "        self.objective_function = objective_function\n",
    "        self.population_size = params.get(\"population_size\", 10)\n",
    "        self.generations = params.get(\"generations\", 20)\n",
    "        self.random_seed = params.get(\"seed\")\n",
    "        if self.random_seed is not None:\n",
    "            random.seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "        self.score = float(\"inf\")\n",
    "\n",
    "    def compute(self):\n",
    "        self.score = self.objective_function(self.sets)\n",
    "\n",
    "    def optimize(self):\n",
    "        population = [safe_deepcopy(self.sets) for _ in range(self.population_size)]\n",
    "        scores = [self.objective_function(p) for p in population]\n",
    "\n",
    "        for _ in range(self.generations):\n",
    "            ranked = sorted(zip(scores, population), key=lambda x: x[0])\n",
    "            population = [x[1] for x in ranked[:self.population_size // 2]]\n",
    "            new_population = []\n",
    "            while len(new_population) < self.population_size:\n",
    "                p1, p2 = random.sample(population, 2)\n",
    "                child = self.crossover(p1, p2)\n",
    "                if random.random() < 0.3:\n",
    "                    self.mutate(child)\n",
    "                new_population.append(child)\n",
    "            population = new_population\n",
    "            scores = [self.objective_function(p) for p in population]\n",
    "\n",
    "        best_idx = np.argmin(scores)\n",
    "        self.sets = population[best_idx]\n",
    "\n",
    "    def crossover(self, p1: SetType, p2: SetType) -> SetType:\n",
    "        return [random.choice([c1, c2]) for c1, c2 in zip(p1, p2)]\n",
    "\n",
    "    def mutate(self, sets: SetType):\n",
    "        if len(sets) < 2:\n",
    "            return\n",
    "        i, j = random.sample(range(len(sets)), 2)\n",
    "        if sets[i] and sets[j]:\n",
    "            a_idx = random.randint(0, len(sets[i]) - 1)\n",
    "            b_idx = random.randint(0, len(sets[j]) - 1)\n",
    "            swap_elements(sets, i, j, a_idx, b_idx)\n",
    "\n",
    "\n",
    "class ContinuousRelaxationOptimizer:\n",
    "    def __init__(self, sets: SetType, objective_function: Callable, params: Dict):\n",
    "        self.sets = sets\n",
    "        self.objective_function = objective_function\n",
    "        self.params = params\n",
    "        self.score = float(\"inf\")\n",
    "\n",
    "    def compute(self):\n",
    "        self.score = self.objective_function(self.sets)\n",
    "\n",
    "    def optimize(self):\n",
    "        all_objects = [obj for subset in self.sets for obj in subset]\n",
    "        if not all_objects:\n",
    "            return\n",
    "\n",
    "        num_clusters = len(self.sets)\n",
    "        prob_matrix = np.random.dirichlet(np.ones(num_clusters), size=len(all_objects))\n",
    "        new_sets = [[] for _ in range(num_clusters)]\n",
    "\n",
    "        for idx, probs in enumerate(prob_matrix):\n",
    "            chosen = np.argmax(probs)\n",
    "            new_sets[chosen].append(all_objects[idx])\n",
    "\n",
    "        self.sets[:] = new_sets\n",
    "\n",
    "class OptimizationPipeline:\n",
    "    def __init__(self, objects: SetType, m: int, objective_function: Callable, verbose: bool = False):\n",
    "        self.objects = objects        \n",
    "        self.m = m\n",
    "        self.objective_function = objective_function\n",
    "        self.steps = []\n",
    "        self.sets = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def add_step(self, optimizer_class: Optimizer, params: Dict = {}):\n",
    "        self.steps.append((optimizer_class, params))\n",
    "    \n",
    "    def run(self):\n",
    "        pipeline_start = time.time()\n",
    "        step_results = []\n",
    "\n",
    "        if not self.steps:\n",
    "            raise ValueError(\"Empty steps are not allowed.\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\n🔍 Iniciando pipeline com {len(self.objects)} objetos no conjunto original.\")\n",
    "\n",
    "        for i, (optimizer_class, params) in enumerate(self.steps):\n",
    "            step_start = time.time()\n",
    "\n",
    "            if i == 0:\n",
    "                if self.verbose:\n",
    "                    print(f\"\\n🚀 Etapa {i+1}: {optimizer_class.__name__} (inicial)\")\n",
    "                optimizer = optimizer_class(self.objects, int(self.m), self.objective_function, params)\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f\"\\n🔄 Etapa {i+1}: {optimizer_class.__name__} (entrada com {len(flatten_sets(self.sets))} objetos)\")\n",
    "                optimizer = optimizer_class(self.sets, self.objective_function, params)\n",
    "\n",
    "            optimizer.optimize()\n",
    "\n",
    "            self.sets = optimizer.sets\n",
    "            step_time = time.time() - step_start\n",
    "            flattened_sets = flatten_sets(self.sets)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"✅ Etapa {i+1} concluída - {len(flattened_sets)} objetos nos conjuntos otimizados.\")\n",
    "                print(f\"📈 Score da métrica ({self.objective_function.__name__}): {optimizer.score:.4f}\")\n",
    "                print(f\"⏱️ Tempo: {step_time:.2f} segundos\")\n",
    "\n",
    "            step_results.append({\n",
    "                \"optimizer\": optimizer_class.__name__,\n",
    "                \"score\": optimizer.score,\n",
    "                \"time\": step_time\n",
    "            })\n",
    "        \n",
    "        final_count = len(flatten_sets(self.sets))\n",
    "        elapsed_time = time.time() - pipeline_start\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\n🧾 Verificação final: {final_count} objetos no total em {elapsed_time} segundos.\")\n",
    "\n",
    "        if not validate_sets(self.objects, self.sets):\n",
    "            raise ValueError(\"❌ Os conjuntos finais não correspondem aos objetos originais!\")\n",
    "\n",
    "        return {\n",
    "            \"sets\": self.sets,\n",
    "            \"score\": self.objective_function(self.sets),\n",
    "            \"time\": elapsed_time,\n",
    "            \"steps\": step_results\n",
    "        }\n",
    "\n",
    "\n",
    "class ClusterOptimizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        fname: str, \n",
    "        original_clusters: List, \n",
    "        pipeline_initializer: Callable[[List[SetType], int], \"OptimizationPipeline\"],\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        self.fname = fname\n",
    "        self.original_clusters = original_clusters\n",
    "        self.pipeline_initializer = pipeline_initializer\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.df = self.load_data()\n",
    "        self.df_original = self.filter_original_clusters()\n",
    "        self.results = {}\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        columns = ['db_name', 'cluster_id', 'size_mb', 'access_daily_count']\n",
    "        dtypes = {\n",
    "            'db_name': str,\n",
    "            'cluster_id': int,\n",
    "            'size_mb': float,\n",
    "            'access_daily_count': int\n",
    "        }\n",
    "\n",
    "        df = pd.read_excel(self.fname, usecols=columns, dtype=dtypes)\n",
    "\n",
    "        duplicated_mask=df.duplicated(subset=\"db_name\", keep=False)\n",
    "        duplicated = df[duplicated_mask]\n",
    "\n",
    "        if not duplicated.empty:\n",
    "            logging.warning(f\"{len(duplicated)} duplicated entries found for 'db_name'. Dropping duplicates and keeping first.\")\n",
    "\n",
    "        return df.drop_duplicates(subset=\"db_name\", keep=\"first\")\n",
    "\n",
    "    def filter_original_clusters(self):\n",
    "        columns=['db_name', 'cluster_id', 'size_mb', 'access_daily_count']\n",
    "        cluster_mask=self.df['cluster_id'].isin(self.original_clusters)\n",
    "        df_original=self.df[cluster_mask][columns]\n",
    "        return df_original\n",
    "\n",
    "    def prepare_sets(self):\n",
    "        return [\n",
    "            (row.db_name, row.size_mb, row.access_daily_count) \n",
    "            for row in self.df_original.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "    def optimize_clusters(self, num_clusters: int):\n",
    "        sets = self.prepare_sets()\n",
    "        pipeline = self.pipeline_initializer(sets, num_clusters)\n",
    "        return pipeline.run()\n",
    "\n",
    "    def map_clusters(self, optimized_df):\n",
    "        optimized_columns=[\"db_name\", \"new_cluster\"]\n",
    "        merge_column=\"db_name\"\n",
    "        comparison_df = self.df_original.merge(\n",
    "            optimized_df[optimized_columns], \n",
    "            on=merge_column, \n",
    "            how=\"left\"\n",
    "        )\n",
    "        \n",
    "        cluster_columns=[\"cluster_id\", \"new_cluster\"]\n",
    "        \n",
    "        cluster_mapping = (\n",
    "            comparison_df.groupby(cluster_columns)['db_name']\n",
    "            .count().unstack(fill_value=0)\n",
    "        )\n",
    "\n",
    "        cluster_renaming = {}\n",
    "        new_clusters = set(comparison_df[\"new_cluster\"].unique())\n",
    "        new_cluster_id = max(self.original_clusters) + 1\n",
    "\n",
    "        for orig in self.original_clusters:\n",
    "            if orig in cluster_mapping.index:\n",
    "                best_match = cluster_mapping.loc[orig].idxmax()\n",
    "                cluster_renaming[best_match] = orig\n",
    "                new_clusters.discard(best_match)\n",
    "\n",
    "        for cluster in cluster_mapping.columns.difference(cluster_renaming.keys()):\n",
    "            cluster_renaming[cluster] = new_cluster_id\n",
    "            new_cluster_id += 1\n",
    "\n",
    "        comparison_df[\"new_cluster_mapped\"] = comparison_df[\"new_cluster\"].map(cluster_renaming)\n",
    "        return comparison_df\n",
    "\n",
    "    def generate_results(self, num_clusters: Optional[int] = None):\n",
    "        min_clusters = len(self.original_clusters)\n",
    "        if num_clusters is None:\n",
    "            num_clusters = min_clusters\n",
    "        elif num_clusters < min_clusters:\n",
    "            raise ValueError(\n",
    "                f\"Provided 'num_clusters' = {num_clusters} is less than the number \"\n",
    "                f\"of original clusters ({min_clusters}). It must be greater than or equal.\"\n",
    "            )\n",
    "\n",
    "        if num_clusters < min_clusters:\n",
    "            raise ValueError(f\"You must provide at least {min_clusters} clusters.\")\n",
    "\n",
    "        for k in range(min_clusters, num_clusters + 1):\n",
    "            result = self.optimize_clusters(k)\n",
    "\n",
    "            optimized_df = pd.DataFrame([\n",
    "                {\"db_name\": db_name, \"size_mb\": size, \"access_daily_count\": access, \"new_cluster\": i}\n",
    "                for i, cluster in enumerate(result[\"sets\"])\n",
    "                for db_name, size, access in cluster\n",
    "            ]).drop_duplicates(subset=\"db_name\", keep=\"first\")\n",
    "\n",
    "            comparison_df = self.map_clusters(optimized_df)\n",
    "            comparison_df[\"status\"] = comparison_df.apply(\n",
    "                lambda row: \"unchanged\" if row[\"cluster_id\"] == row[\"new_cluster_mapped\"] else \"changed\",\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            comparison_df[\"movement\"] = comparison_df.apply(\n",
    "                lambda row: f\"{row['cluster_id']} ➝ {row['new_cluster_mapped']}\", axis=1\n",
    "            )\n",
    "\n",
    "            # Inclui TODOS os bancos, inclusive os que permaneceram\n",
    "            movement_stats = comparison_df.groupby(\"movement\").agg(total_dbs=(\"db_name\", \"count\")).reset_index()\n",
    "\n",
    "            new_cluster_metrics = comparison_df.groupby(\"new_cluster_mapped\").agg(\n",
    "                total_size=(\"size_mb\", \"sum\"),\n",
    "                total_accesses=(\"access_daily_count\", \"sum\"),\n",
    "                db_count=(\"db_name\", \"count\")\n",
    "            ).reset_index()\n",
    "\n",
    "            self.results[k] = {\n",
    "                \"comparison_df\": comparison_df,\n",
    "                \"movement_stats\": movement_stats,\n",
    "                \"new_cluster_metrics\": new_cluster_metrics,\n",
    "            }\n",
    "\n",
    "    def display_results(self):\n",
    "        total_size_mb = self.df_original[\"size_mb\"].sum()\n",
    "        total_access_count = self.df_original[\"access_daily_count\"].sum()\n",
    "\n",
    "        print(f\"\\nDatabases count: {len(self.df_original)}\")\n",
    "        print(f\"📦 Total size across all clusters: {total_size_mb:,.2f} MB\")\n",
    "        print(f\"🔐 Total daily accesses across all clusters: {total_access_count:,}\")\n",
    "\n",
    "        for k, data in self.results.items():\n",
    "            print(f\"\\n🔍 Results for {k} clusters:\")\n",
    "            comparison_df = data[\"comparison_df\"]\n",
    "\n",
    "            print(\"\\n📊 Movement statistics:\")\n",
    "            movement_stats = data[\"movement_stats\"].copy()\n",
    "            movement_stats[['from_cluster', 'to_cluster']] = movement_stats['movement'].str.extract(r'(\\d+)\\s*➝\\s*(\\d+)').astype(int)\n",
    "            movement_stats.sort_values(by=['from_cluster', 'to_cluster'], inplace=True)\n",
    "            movement_stats.drop(columns=['from_cluster', 'to_cluster'], inplace=True)\n",
    "            print(tabulate(movement_stats, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
    "\n",
    "            total_dbs = comparison_df[\"db_name\"].nunique()\n",
    "            moved_dbs = comparison_df.query(\"status == 'changed'\")[\"db_name\"].nunique()\n",
    "\n",
    "            print(f\"\\n📦 Total DBs: {total_dbs:,}\")\n",
    "            print(f\"🚚 Moved DBs: {moved_dbs:,}\")\n",
    "\n",
    "            print(\"\\n📊 Metrics for new clusters:\")\n",
    "            cluster_metrics = data[\"new_cluster_metrics\"].copy()\n",
    "            cluster_metrics[\"size_pct\"] = (cluster_metrics[\"total_size\"] / total_size_mb * 100).round(2)\n",
    "            cluster_metrics[\"access_pct\"] = (cluster_metrics[\"total_accesses\"] / total_access_count * 100).round(2)\n",
    "            cluster_metrics.sort_values(by=\"new_cluster_mapped\", inplace=True)\n",
    "            print(tabulate(cluster_metrics, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
    "        \n",
    "    def save_results(self, output_dir: str = \".\", base_filename: str = \"cluster_result\"):\n",
    "        \"\"\"\n",
    "        Salva os resultados da otimização de clusters em arquivos Excel.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str): Diretório onde os arquivos serão salvos.\n",
    "            base_filename (str): Prefixo dos arquivos Excel.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for k, data in self.results.items():\n",
    "            df = data[\"comparison_df\"].copy()\n",
    "            df = df.rename(columns={\"cluster_id\": \"from_cluster\", \"new_cluster_mapped\": \"to_cluster\"})\n",
    "            df = df[[\"db_name\", \"from_cluster\", \"to_cluster\"]].sort_values(by=\"db_name\")\n",
    "\n",
    "            output_path = os.path.join(output_dir, f\"{base_filename}_{k}_clusters.xlsx\")\n",
    "            df.to_excel(output_path, index=False)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[✔] Resultado salvo: {output_path}\")\n",
    "\n",
    "def weighted_balanced_mad_metric(\n",
    "    sets: List[List[Tuple]],\n",
    "    weights: Optional[List[float]] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Métrica de balanceamento multivariada com MAD normalizado ponderado.\n",
    "\n",
    "    Args:\n",
    "        sets: Lista de conjuntos, cada um com objetos como tuplas/listas indexáveis.\n",
    "        weights: Pesos para cada feature. Se None, usa média simples.\n",
    "\n",
    "    Returns:\n",
    "        Score médio ponderado das MADs normalizadas.\n",
    "    \"\"\"\n",
    "    if not sets or not any(sets):\n",
    "        return 0.0\n",
    "\n",
    "    indices = list(range(1, len(sets[0][0])))  # ignora o índice 0 (assumido como ID)\n",
    "\n",
    "    set_vectors = [\n",
    "        np.sum([[obj[i] for i in indices] for obj in s], axis=0)\n",
    "        for s in sets\n",
    "    ]\n",
    "    set_vectors = np.array(set_vectors)\n",
    "\n",
    "    medians = np.median(set_vectors, axis=0)\n",
    "    abs_devs = np.abs(set_vectors - medians)\n",
    "    mad = np.median(abs_devs, axis=0)\n",
    "    norm_mad = np.where(medians != 0, mad / medians, 0)\n",
    "\n",
    "    if weights is not None:\n",
    "        weights = np.array(weights, dtype=float)\n",
    "\n",
    "        if len(weights) != len(indices):\n",
    "            raise ValueError(f\"Número de pesos ({len(weights)}) não bate com o número de features ({len(indices)})\")\n",
    "\n",
    "        if np.any(weights < 0):\n",
    "            raise ValueError(\"Todos os pesos devem ser não-negativos\")\n",
    "\n",
    "        if weights.sum() == 0:\n",
    "            raise ValueError(\"A soma dos pesos deve ser maior que zero\")\n",
    "\n",
    "        weights = weights / weights.sum()\n",
    "        return float(np.sum(norm_mad * weights))\n",
    "\n",
    "    return float(np.mean(norm_mad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_REGISTRY = {}\n",
    "\n",
    "def register_pipeline(name):\n",
    "    def wrapper(fn):\n",
    "        PIPELINE_REGISTRY[name] = fn\n",
    "        return fn\n",
    "    return wrapper\n",
    "\n",
    "def with_default_clusters(fn):\n",
    "    def wrapper(sets, num_clusters=None):\n",
    "        if num_clusters is None:\n",
    "            num_clusters = len(getattr(sets, \"original_clusters\", sets))\n",
    "        return fn(sets, num_clusters)\n",
    "    return wrapper\n",
    "\n",
    "@register_pipeline('default')\n",
    "@with_default_clusters\n",
    "def make_pipeline_default(sets, num_clusters):\n",
    "    def objective_function(sets):\n",
    "        return weighted_balanced_mad_metric(sets, weights=[0.4, 0.6])\n",
    "    \n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective_function, verbose=True)\n",
    "    pipeline.add_step(GreedyOptimizer)\n",
    "    pipeline.add_step(SwapOptimizer, {\"max_iterations\": 1000})\n",
    "    pipeline.add_step(SimulatedAnnealingOptimizer)\n",
    "    pipeline.add_step(SwapOptimizer, {\"max_iterations\": 1000})\n",
    "    return pipeline\n",
    "\n",
    "@register_pipeline('basic')\n",
    "@with_default_clusters\n",
    "def make_pipeline_basic(sets, num_clusters):\n",
    "    def objective(sets): return weighted_balanced_mad_metric(sets, [0.5, 0.5])\n",
    "\n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective, verbose=True)\n",
    "    pipeline.add_step(GreedyOptimizer)\n",
    "    pipeline.add_step(SwapOptimizer, {\"max_iterations\": 500})\n",
    "    return pipeline\n",
    "\n",
    "@register_pipeline('metaheuristic')\n",
    "@with_default_clusters\n",
    "def make_pipeline_metaheuristic(sets, num_clusters):\n",
    "    def objective(sets): return weighted_balanced_mad_metric(sets, [0.4, 0.6])\n",
    "\n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective, verbose=True)\n",
    "    pipeline.add_step(GreedyOptimizer)\n",
    "    pipeline.add_step(SimulatedAnnealingOptimizer, {\n",
    "        \"max_iterations\": 1500,\n",
    "        \"initial_temp\": 20.0,\n",
    "        \"cooling_rate\": 0.99\n",
    "    })\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "@register_pipeline('refinement')\n",
    "@with_default_clusters\n",
    "def make_pipeline_refinement(sets, num_clusters):\n",
    "    def objective(sets): return weighted_balanced_mad_metric(sets, [0.5, 0.5])\n",
    "\n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective, verbose=False)\n",
    "    pipeline.add_step(GreedyOptimizer)\n",
    "    pipeline.add_step(KernighanLinOptimizer, {})\n",
    "    pipeline.add_step(MigrationOptimizer, {})\n",
    "    return pipeline\n",
    "\n",
    "@register_pipeline('exhaustive')\n",
    "@with_default_clusters\n",
    "def make_pipeline_exhaustive(sets, num_clusters):\n",
    "    def objective(sets): return weighted_balanced_mad_metric(sets, [0.3, 0.7])\n",
    "\n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective, verbose=True)\n",
    "    pipeline.add_step(GreedyOptimizer)\n",
    "    pipeline.add_step(GeneticOptimizer, {\n",
    "        \"population_size\": 25,\n",
    "        \"generations\": 50,\n",
    "        \"seed\": 1337\n",
    "    })\n",
    "    pipeline.add_step(SimulatedAnnealingOptimizer, {\n",
    "        \"max_iterations\": 2000,\n",
    "        \"initial_temp\": 30.0,\n",
    "        \"cooling_rate\": 0.995\n",
    "    })\n",
    "    pipeline.add_step(TabuSearchOptimizer, {\n",
    "        \"tabu_size\": 100,\n",
    "        \"max_iterations\": 400\n",
    "    })\n",
    "    pipeline.add_step(MigrationOptimizer, {})\n",
    "    return pipeline\n",
    "\n",
    "@register_pipeline('genetic_relaxation')\n",
    "@with_default_clusters\n",
    "def make_pipeline_genetic_relaxation(sets, num_clusters):\n",
    "    def objective(sets): return weighted_balanced_mad_metric(sets, [0.6, 0.4])\n",
    "\n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective, verbose=True)\n",
    "    pipeline.add_step(GeneticOptimizer, {\n",
    "        \"population_size\": 20,\n",
    "        \"generations\": 30,\n",
    "        \"seed\": 42\n",
    "    })\n",
    "    pipeline.add_step(ContinuousRelaxationOptimizer, {})\n",
    "    return pipeline\n",
    "\n",
    "@register_pipeline('fast')\n",
    "@with_default_clusters\n",
    "def make_pipeline_fast(sets, num_clusters):\n",
    "    def objective_function(sets):\n",
    "        return weighted_balanced_mad_metric(sets, weights=[0.5, 0.5])\n",
    "    \n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective_function, verbose=False)\n",
    "    pipeline.add_step(GreedyOptimizer)\n",
    "    pipeline.add_step(SwapOptimizer, {\"max_iterations\": 300})\n",
    "    return pipeline\n",
    "\n",
    "@register_pipeline('minimal')\n",
    "@with_default_clusters\n",
    "def make_pipeline_minimal(sets, num_clusters):\n",
    "    def objective_function(sets):\n",
    "        return weighted_balanced_mad_metric(sets, weights=[0.5, 0.5])\n",
    "    \n",
    "    pipeline = OptimizationPipeline(sets, num_clusters, objective_function, verbose=False)\n",
    "    pipeline.add_step(GreedyOptimizer)\n",
    "    return pipeline\n",
    "\n",
    "def get_pipeline(name: str = 'default', verbose: bool = False):\n",
    "    if name not in PIPELINE_REGISTRY:\n",
    "        available = ', '.join(sorted(PIPELINE_REGISTRY.keys()))\n",
    "        raise ValueError(\n",
    "            f\"Pipeline '{name}' is not registered.\\n\"\n",
    "            f\"Available pipelines are: {available}\"\n",
    "        )\n",
    "\n",
    "    def wrapped_pipeline(sets, num_clusters=None):\n",
    "        pipeline = PIPELINE_REGISTRY[name](sets, num_clusters)\n",
    "        pipeline.verbose = verbose  # force override after construction\n",
    "        return pipeline\n",
    "\n",
    "    return wrapped_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PIPELINE_REGISTRY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m OUTPUT_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_test_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m CLUSTER_COUNTS_TO_TEST \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m AVAILABLE_PIPELINES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     13\u001b[0m OUTPUT_DIR\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_optimizer_test\u001b[39m(pipeline_name: \u001b[38;5;28mstr\u001b[39m, cluster_count: \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PIPELINE_REGISTRY' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import traceback\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações iniciais\n",
    "FNAME = \"clusters_20250404.xlsx\"\n",
    "ORIGINAL_CLUSTERS = [5438, 5439]\n",
    "OUTPUT_DIR = Path(\"pipeline_test_results\")\n",
    "CLUSTER_COUNTS_TO_TEST = [2]\n",
    "AVAILABLE_PIPELINES = list(PIPELINE_REGISTRY.keys())\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def run_optimizer_test(pipeline_name: str, cluster_count: int):\n",
    "    print(f\"\\n[TESTANDO] Pipeline: '{pipeline_name}' | Clusters: {cluster_count}\")\n",
    "    try:\n",
    "        selected_pipeline = get_pipeline(pipeline_name, verbose=False)\n",
    "\n",
    "        optimizer = ClusterOptimizer(\n",
    "            fname=FNAME,\n",
    "            original_clusters=ORIGINAL_CLUSTERS,\n",
    "            pipeline_initializer=selected_pipeline,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        optimizer.generate_results(num_clusters=cluster_count)\n",
    "        optimizer.display_results()\n",
    "\n",
    "        result_path = OUTPUT_DIR / f\"{pipeline_name}_{cluster_count}clusters.xlsx\"\n",
    "        optimizer.save_results(result_path)\n",
    "\n",
    "        print(f\"[✔] Resultado salvo em: {result_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[✘] Erro com pipeline '{pipeline_name}' e {cluster_count} clusters.\")\n",
    "        print(\"Detalhes do erro:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def main(pipelines: List[str], cluster_counts: List[int]):\n",
    "    for pipeline in pipelines:\n",
    "        for count in cluster_counts:\n",
    "            run_optimizer_test(pipeline, count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(AVAILABLE_PIPELINES, CLUSTER_COUNTS_TO_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
