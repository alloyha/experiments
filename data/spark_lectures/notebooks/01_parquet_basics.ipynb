{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Basics and Layout Strategies\n",
    "\n",
    "This notebook demonstrates fundamental Parquet optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Parquet Basics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark {spark.version} ready!\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample sales data\n",
    "df = spark.range(0, 1_000_000) \\\n",
    "    .select(\n",
    "        (col(\"id\") % 100000).alias(\"customer_id\"),\n",
    "        (col(\"id\") % 10000).alias(\"product_id\"),\n",
    "        expr(\"date_add('2024-01-01', cast(id % 365 as int))\").alias(\"sale_date\"),\n",
    "        (rand() * 1000).alias(\"amount\"),\n",
    "        (col(\"id\") % 5).cast(\"int\").alias(\"region_id\")\n",
    "    ) \\\n",
    "    .withColumn(\"region\", \n",
    "        when(col(\"region_id\") == 0, \"US\")\n",
    "        .when(col(\"region_id\") == 1, \"EU\")\n",
    "        .when(col(\"region_id\") == 2, \"ASIA\")\n",
    "        .when(col(\"region_id\") == 3, \"LATAM\")\n",
    "        .otherwise(\"OTHER\")\n",
    "    ) \\\n",
    "    .withColumn(\"year\", year(\"sale_date\")) \\\n",
    "    .withColumn(\"month\", month(\"sale_date\"))\n",
    "\n",
    "df.show(5)\n",
    "print(f\"Generated {df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write unpartitioned Parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://warehouse/sales_unpartitioned\")\n",
    "print(\"✓ Unpartitioned written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned by year/month\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3a://warehouse/sales_partitioned\")\n",
    "print(\"✓ Partitioned written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare query performance\n",
    "import time\n",
    "\n",
    "# Unpartitioned query\n",
    "unpart = spark.read.parquet(\"s3a://warehouse/sales_unpartitioned\")\n",
    "t0 = time.time()\n",
    "count_unpart = unpart.filter(\"month = 6\").count()\n",
    "t_unpart = time.time() - t0\n",
    "\n",
    "# Partitioned query\n",
    "part = spark.read.parquet(\"s3a://warehouse/sales_partitioned\")\n",
    "t0 = time.time()\n",
    "count_part = part.filter(\"month = 6\").count()\n",
    "t_part = time.time() - t0\n",
    "\n",
    "print(f\"Unpartitioned: {count_unpart:,} rows in {t_unpart:.3f}s\")\n",
    "print(f\"Partitioned:   {count_part:,} rows in {t_part:.3f}s\")\n",
    "print(f\"Speedup: {t_unpart/t_part:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show physical plans\n",
    "print(\"\\nUnpartitioned plan:\")\n",
    "unpart.filter(\"month = 6\").explain()\n",
    "\n",
    "print(\"\\nPartitioned plan:\")\n",
    "part.filter(\"month = 6\").explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
