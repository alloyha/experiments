# Spark Configuration for Comprehensive Table Format Learning
# Place this file in: ./conf/spark-defaults.conf

# ============================================================================
# S3/MinIO Configuration
# ============================================================================
spark.hadoop.fs.s3a.endpoint                    http://minio:9000
spark.hadoop.fs.s3a.access.key                  admin
spark.hadoop.fs.s3a.secret.key                  password
spark.hadoop.fs.s3a.path.style.access           true
spark.hadoop.fs.s3a.impl                        org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled      false
spark.hadoop.fs.s3a.aws.credentials.provider    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# ============================================================================
# Hive Metastore Configuration
# ============================================================================
spark.sql.warehouse.dir                         s3a://warehouse/
spark.hive.metastore.uris                       thrift://hive-metastore:9083
spark.sql.catalogImplementation                 hive

# ============================================================================
# Delta Lake & Iceberg Configuration (without Hudi to avoid resolution issues)
# ============================================================================
spark.sql.extensions                            io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtension
spark.sql.catalog.spark_catalog                 org.apache.spark.sql.delta.catalog.DeltaCatalog

# Delta optimizations
spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite    true
spark.databricks.delta.properties.defaults.autoOptimize.autoCompact      true
spark.databricks.delta.optimizeWrite.enabled                             true
spark.databricks.delta.autoCompact.enabled                               true

# ============================================================================
# Iceberg Configuration
# ============================================================================
spark.sql.catalog.iceberg                       org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type                  rest
spark.sql.catalog.iceberg.uri                   http://iceberg-rest:8181
spark.sql.catalog.iceberg.io-impl               org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.iceberg.warehouse             s3a://iceberg/
spark.sql.catalog.iceberg.s3.endpoint           http://minio:9000
spark.sql.catalog.iceberg.s3.path-style-access  true

# Iceberg Hive catalog (alternative)
spark.sql.catalog.iceberg_hive                  org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg_hive.type             hive
spark.sql.catalog.iceberg_hive.uri              thrift://hive-metastore:9083
spark.sql.catalog.iceberg_hive.warehouse        s3a://iceberg/

# ============================================================================
# ============================================================================
# Performance Tuning
# ============================================================================
# Adaptive Query Execution
spark.sql.adaptive.enabled                      true
spark.sql.adaptive.coalescePartitions.enabled   true
spark.sql.adaptive.skewJoin.enabled             true

# Memory
spark.driver.memory                             4g
spark.executor.memory                           4g
spark.driver.memoryOverhead                     1g
spark.executor.memoryOverhead                   1g

# Shuffle
spark.sql.shuffle.partitions                    100
spark.default.parallelism                       8

# Network
spark.network.timeout                           600s

# ============================================================================
# Parquet Optimization
# ============================================================================
spark.sql.parquet.compression.codec             snappy
spark.sql.parquet.filterPushdown                true
spark.sql.parquet.mergeSchema                   false
spark.sql.files.maxPartitionBytes               128MB

# ============================================================================
# UI and Logging
# ============================================================================
spark.ui.enabled                                true
spark.eventLog.enabled                          false

# ============================================================================
# Bucketing and Partitioning
# ============================================================================
spark.sql.sources.bucketing.enabled             true

# ============================================================================
# Additional JARs (WITHOUT Hudi to avoid Maven resolution failures)
# ============================================================================
spark.jars.packages                             io.delta:delta-spark_2.12:3.2.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262