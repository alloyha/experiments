{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94fe7346-6ea3-487e-bc87-d75a1015f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from timy import timer\n",
    "import polars as pl\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf262af3-4381-4da8-a940-c130a076dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify_items(\n",
    "    df_: pd.DataFrame,\n",
    "    set_column: str,\n",
    "    item_column: str\n",
    "):\n",
    "    result = df_.groupby(set_column)[item_column] \\\n",
    "               .apply(list) \\\n",
    "               .reset_index(name='items_list')\n",
    "\n",
    "    return list(result['items_list'])\n",
    "\n",
    "def get_unique_elements(\n",
    "    df_: pd.DataFrame,\n",
    "    column_label: str\n",
    "):\n",
    "    return unique(list(df_[column_label]))\n",
    "\n",
    "def get_dataframe_last_n_days(\n",
    "    df_: pd.DataFrame,\n",
    "    time_column: str\n",
    "):\n",
    "    df_.sort_values(by=time_column)\n",
    "\n",
    "    # Assuming 'pedi_data' is in string format, convert it to datetime\n",
    "    df_[time_column] = pd.to_datetime(df_[time_column])\n",
    "\n",
    "    # Get the maximum date\n",
    "    end_date = df[time_column].max()\n",
    "\n",
    "    # Calculate the start date as 30 days before the end date\n",
    "    start_date = end_date - timedelta(days=last_days)\n",
    "\n",
    "    # Filter the DataFrame to include only rows within the last 30 days\n",
    "    interval_mask = (df_[time_column] >= start_date) & (df_[time_column] <= end_date)\n",
    "\n",
    "    return df_[interval_mask]\n",
    "\n",
    "def read_data_to_dataframe_gen(\n",
    "    data_folder_: str,\n",
    "    set_column: str,\n",
    "    item_column: str,\n",
    "    extension: str = 'xlsx'\n",
    "):\n",
    "    filepaths = [\n",
    "        path.join(data_folder, filename) \n",
    "        for filename in listdir(data_folder) \n",
    "        if filename.split('.')[-1] == extension\n",
    "    ]\n",
    "    \n",
    "    dfs = []\n",
    "    for filepath in filepaths:\n",
    "        df_ = pd.read_excel(filepath)\n",
    "\n",
    "        df_p = pl.from_pandas(df_)\n",
    "\n",
    "        # Group by 'pedi_id' and 'prod_id', and select the first occurrence of each group\n",
    "        relevant_columns = [set_column, item_column]\n",
    "        filtered_df = df_p.group_by(relevant_columns).first()\n",
    "\n",
    "        df_ = filtered_df.to_pandas()        \n",
    "        \n",
    "        yield filepath, df_\n",
    "\n",
    "@timer()\n",
    "def read_data_to_dataframe(\n",
    "    data_folder_: str,\n",
    "    set_column: str,\n",
    "    item_column: str,\n",
    "    extension: str = 'xlsx'\n",
    "):\n",
    "    return dict(read_data_to_dataframe_gen(data_folder_, set_column, item_column, extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "631f1531-ded7-457a-ab28-5419de3fc862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timy executed (read_data_to_dataframe) for 1 time in 145.443559 seconds\n",
      "Timy best time was 145.443559 seconds\n"
     ]
    }
   ],
   "source": [
    "from os import getcwd, listdir, path \n",
    "\n",
    "data_folder = getcwd()+'/data/'\n",
    "\n",
    "filename_dfs = dict(read_data_to_dataframe(data_folder, 'pedi_id', 'prod_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd09a0dd-1959-4277-9e34-b05d3da4814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_days = 1\n",
    "\n",
    "filtered_dfs = dict()\n",
    "for filename, df in filename_dfs.items(): \n",
    "    df = get_dataframe_last_n_days(df, 'pedi_data')\n",
    "    len(df)\n",
    "    filtered_dfs[filename] = df\n",
    "\n",
    "dfs = list(filtered_dfs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ff685-99d9-48c0-a0e5-d267dea1f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the apriori modules from mlxtend\n",
    "from mlxtend.frequent_patterns import apriori,association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "'''\n",
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]\n",
    "'''\n",
    "\n",
    "dataset = listify_items(dfs[0], 'pedi_id', 'prod_id')\n",
    "\n",
    "encoded_data = TransactionEncoder()\n",
    "encoded_data = encoded_data.fit(dataset)\\\n",
    "                           .transform(dataset, sparse=True)\n",
    "\n",
    "sparse_df = pd.DataFrame.sparse.from_spmatrix(encoded_data)\n",
    "\n",
    "# Run the association rules function of mined frequent itemset\n",
    "sparse_frequent_candidates = apriori(sparse_df, min_support=0.001, use_colnames=True)\n",
    "\n",
    "# Run the association rules function of mined frequent itemset\n",
    "sparse_rules = association_rules(sparse_frequent_candidates, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Inspect your rules with filters\n",
    "sparse_rules[ (sparse_rules['lift'] >= 4) & (sparse_rules['confidence'] >= 0.5) ]\n",
    "\n",
    "# lets check the size of the standard object and the sparse object we created.\n",
    "# The only reason we are doing the sparse is for space and memory optimization.\n",
    "# import the system module\n",
    "import sys\n",
    "\n",
    "# get size of the two objects to compare\n",
    "sys.getsizeof(sparse_df)\n",
    "sys.getsizeof(sparse_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
