{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "    \"pinecone\" \\\n",
    "    \"langchain-pinecone\" \\\n",
    "    \"langchain-openai\" \\\n",
    "    \"langchain-text-splitters\" \\\n",
    "    \"langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from os import path, makedirs, walk\n",
    "from uuid import uuid4\n",
    "from typing import List, Optional\n",
    "from transformers import pipeline\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec, Index\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_pinecone import PineconeEmbeddings, PineconeVectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from timy import timer\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "\n",
    "# Add a handler to output logs to stdout\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Default values\n",
    "DEFAULT_CLOUD='aws'\n",
    "DEFAULT_REGION='us-east-1'\n",
    "DEFAULT_TOP_K=5\n",
    "DEFAULT_NUM_WORKERS=5\n",
    "DEFAULT_LANGUAGE='pt'\n",
    "DEFAULT_RESULT_TYPE=\"markdown\"\n",
    "DEFAULT_PINECONE_MODEL_NAME='multilingual-e5-large'\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "def get_files_with_extension(folder_paths: List[str], extension: str) -> List[str]:\n",
    "    files_with_extension = []\n",
    "    \n",
    "    for folder in folder_paths:\n",
    "        for root, _, files in walk(folder):\n",
    "            for file in files:\n",
    "                if file.endswith(extension):\n",
    "                    files_with_extension.append(path.join(root, file))\n",
    "\n",
    "    return files_with_extension\n",
    "\n",
    "def create_vectors(data, embeddings):\n",
    "    return [\n",
    "        {\"id\": d[\"id\"], \"values\": e[\"values\"], \"metadata\": {\"text\": d[\"text\"]}}\n",
    "        for d, e in zip(data, embeddings)\n",
    "    ]\n",
    "\n",
    "def safe_execution(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in {func.__name__}: {e}\")\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "class BaseConfig:\n",
    "    def __init__(self, api_key: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Base configuration class to handle common API key and extra parameters.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "class PineconeConfig(BaseConfig):\n",
    "    def __init__(\n",
    "        self, \n",
    "        api_key: str, \n",
    "        model_name: str = DEFAULT_PINECONE_MODEL_NAME, \n",
    "        cloud: str = DEFAULT_CLOUD, \n",
    "        region: str = DEFAULT_REGION\n",
    "    ):\n",
    "        super().__init__(api_key=api_key, model_name=model_name, cloud=cloud, region=region)\n",
    "\n",
    "\n",
    "class ChatOpenAIConfig(BaseConfig):\n",
    "    def __init__(\n",
    "        self, \n",
    "        api_key: str, \n",
    "        temperature: float = 0.0, \n",
    "        model_name: str = DEFAULT_OPENAI_MODEL\n",
    "    ):\n",
    "        super().__init__(api_key=api_key, temperature=temperature, model_name=model_name)\n",
    "\n",
    "\n",
    "class LlamaConfig(BaseConfig):\n",
    "    def __init__(\n",
    "        self, \n",
    "        api_key: str,\n",
    "        result_type: str = DEFAULT_RESULT_TYPE,\n",
    "        num_workers: int = DEFAULT_NUM_WORKERS,\n",
    "        verbose: bool = True,\n",
    "        language: str = DEFAULT_LANGUAGE\n",
    "    ):\n",
    "        super().__init__(api_key=api_key, result_type=result_type, num_workers=num_workers, verbose=verbose, language=language)\n",
    "\n",
    "\n",
    "class ConfigFactory:\n",
    "    @staticmethod\n",
    "    def create_pinecone_config(api_key: Optional[str] = None) -> PineconeConfig:\n",
    "        api_key = api_key or os.getenv('PINECONE_API_KEY')\n",
    "        return PineconeConfig(api_key=api_key)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_chat_openai_config(api_key: Optional[str] = None) -> ChatOpenAIConfig:\n",
    "        api_key = api_key or os.getenv('OPENAI_API_KEY')\n",
    "        return ChatOpenAIConfig(api_key=api_key)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_llama_config(api_key: Optional[str] = None) -> LlamaConfig:\n",
    "        api_key = api_key or os.getenv('LLAMA_API_KEY')\n",
    "        return LlamaConfig(api_key=api_key)\n",
    "\n",
    "\n",
    "def basename_without_extension(filepath: str): \n",
    "    return path.splitext(path.basename(filepath))[0]\n",
    "\n",
    "def split_pdf_in_chunks(input_pdf_path: str, output_folder: str, pages_per_chunk: int):\n",
    "    # Ensure the output folder exists\n",
    "    makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    basename=basename_without_extension(input_pdf_path)\n",
    "    \n",
    "    # Load the PDF\n",
    "    pdf_reader = PdfReader(input_pdf_path)\n",
    "    total_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Process the PDF in chunks\n",
    "    output_paths=[]\n",
    "    for start_page in range(0, total_pages, pages_per_chunk):\n",
    "        pdf_writer = PdfWriter()\n",
    "        end_page = min(start_page + pages_per_chunk, total_pages)\n",
    "\n",
    "        # Add specified number of pages to the writer\n",
    "        for page_num in range(start_page, end_page):\n",
    "            page=pdf_reader.pages[page_num]\n",
    "            pdf_writer.add_page(page)\n",
    "\n",
    "        # Define the output path for each chunk\n",
    "        chunk_number = start_page // pages_per_chunk + 1\n",
    "        filename = f'{basename}_chunk_{chunk_number}.pdf'\n",
    "        output_path = path.join(output_folder, filename)\n",
    "        \n",
    "        # Save the chunk as a PDF\n",
    "        with open(output_path, 'wb') as output_pdf:\n",
    "            pdf_writer.write(output_pdf)\n",
    "\n",
    "        output_paths.append(output_path)\n",
    "\n",
    "    return output_paths\n",
    "\n",
    "class PDFToMarkdownParser:\n",
    "    def __init__(self, llama_config: LlamaConfig):\n",
    "        self.llama_config=llama_config\n",
    "        self._llama_parser=LlamaParse(\n",
    "            api_key=llama_config.api_key,\n",
    "            result_type=llama_config.result_type,\n",
    "            num_workers=llama_config.num_workers,\n",
    "            verbose=llama_config.verbose,\n",
    "            language=llama_config.language,\n",
    "        )\n",
    "\n",
    "    @timer()\n",
    "    def parse(self, filename: str):\n",
    "        \"\"\"Parses PDF content to Markdown.\"\"\"\n",
    "        qa_model = pipeline(\"question-answering\")\n",
    "        embedder = pipeline(\"feature-extraction\")\n",
    "        parsed_data = self._llama_parser.load_data(filename)\n",
    "        print(f\"Parsing of data {filename} is ready!\")\n",
    "        return parsed_data\n",
    "\n",
    "class MarkdownComprehender:\n",
    "    def __init__(\n",
    "        self, \n",
    "        pinecone_config: PineconeConfig,\n",
    "        chat_openai_config: ChatOpenAIConfig\n",
    "    ):\n",
    "        # Pinecone references\n",
    "        self.pinecone_config = pinecone_config\n",
    "        self.chat_openai_config = chat_openai_config\n",
    "        self._pinecone = Pinecone(api_key=pinecone_config.api_key)\n",
    "        self._embeddings = PineconeEmbeddings(\n",
    "            model=pinecone_config.model_name, \n",
    "            pinecone_api_key=pinecone_config.api_key\n",
    "        )\n",
    "        self.llm = ChatOpenAI(\n",
    "            openai_api_key=self.chat_openai_config.api_key, \n",
    "            model_name=self.chat_openai_config.model_name, \n",
    "            temperature=self.chat_openai_config.temperature\n",
    "        )\n",
    "        self.combine_docs_chain = create_stuff_documents_chain(self.llm, retrieval_qa_chat_prompt)\n",
    "\n",
    "    # Pinecone index\n",
    "    def _get_pinecone_spec(self):\n",
    "        return ServerlessSpec(\n",
    "            cloud=self.pinecone_config.cloud, \n",
    "            region=self.pinecone_config.region\n",
    "        )\n",
    "\n",
    "    @safe_execution\n",
    "    def _get_index(self, index_name: str) -> Optional[Index]:\n",
    "        if index_name not in self._pinecone.list_indexes().names():\n",
    "            self._pinecone.create_index(\n",
    "                name=index_name, \n",
    "                dimension=self._embeddings.dimension, \n",
    "                metric=\"cosine\",\n",
    "                spec=self._get_pinecone_spec()\n",
    "            )\n",
    "            while not self._pinecone.describe_index(index_name).status['ready']:\n",
    "                time.sleep(1)\n",
    "        return self._pinecone.Index(index_name)\n",
    "\n",
    "    def create_embeddings(self, texts: List[str]):\n",
    "        return self._pinecone.inference.embed(\n",
    "            model=self.pinecone_config.model_name, inputs=texts, parameters={\"input_type\": \"passage\"}\n",
    "        )\n",
    "    \n",
    "    def markdown_to_document(self, markdown_text: str):\n",
    "        headers_to_split_on = [ (\"##\", \"Header 2\") ]\n",
    "\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "        )\n",
    "        return markdown_splitter.split_text(markdown_text)[0]\n",
    "\n",
    "    @timer()\n",
    "    def upsert(self, namespace: str, index_name: str, texts: List[str]):\n",
    "        data = [{\"id\": str(uuid4()), \"text\": text} for text in texts]\n",
    "        embeddings = self.create_embeddings([d[\"text\"] for d in data])\n",
    "        vectors = create_vectors(data, embeddings)\n",
    "        index = self._get_index(index_name)\n",
    "        return index.upsert(vectors=vectors, namespace=namespace)\n",
    "\n",
    "\n",
    "    @timer()\n",
    "    def query(self, namespace: str, index_name: str, query_str: str):\n",
    "        \"\"\"Queries Pinecone index and retrieves relevant documents.\"\"\"\n",
    "        index = self._get_index(index_name)\n",
    "        doc_search = PineconeVectorStore(index_name=index_name, embedding=self._embeddings, namespace=namespace)\n",
    "        retriever = doc_search.as_retriever()\n",
    "        retrieval_chain = create_retrieval_chain(retriever, self.combine_docs_chain)\n",
    "        return retrieval_chain.invoke({\"input\": query_str})\n",
    "\n",
    "\n",
    "class Chat:\n",
    "    def __init__(self, pinecone_config, llama_config, openai_config, index_name, namespace):\n",
    "        \"\"\"\n",
    "        Initialize the Chat class with configuration details for Pinecone, Llama, and OpenAI.\n",
    "\n",
    "        :param pinecone_config: Pinecone configuration object\n",
    "        :param llama_config: Llama configuration object\n",
    "        :param openai_config: OpenAI configuration object\n",
    "        :param index_name: Name of the Pinecone index\n",
    "        :param namespace: Namespace in the vector database\n",
    "        \"\"\"\n",
    "        self.pinecone_config = pinecone_config\n",
    "        self.llama_config = llama_config\n",
    "        self.openai_config = openai_config\n",
    "        self.index_name = index_name\n",
    "        self.namespace = namespace\n",
    "        self.comprehender = MarkdownComprehender(pinecone_config, openai_config)\n",
    "        self.pdf_parser = PDFToMarkdownParser(llama_config)\n",
    "\n",
    "    def parse_document(self, filename: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Parse a PDF document and extract text chunks.\n",
    "\n",
    "        :param filename: Path to the PDF file\n",
    "        :return: List of text chunks\n",
    "        \"\"\"\n",
    "        documents = self.pdf_parser.parse(filename)\n",
    "        texts = [doc.text for doc in documents]\n",
    "        return texts\n",
    "\n",
    "    def upsert_texts(self, texts: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Upsert text chunks into the vector database.\n",
    "\n",
    "        :param texts: List of text chunks\n",
    "        \"\"\"\n",
    "        self.comprehender.upsert(self.namespace, self.index_name, texts)\n",
    "\n",
    "    def query(self, system_query: str, human_query: str) -> str:\n",
    "        \"\"\"\n",
    "        Query the model with the given template and parameters.\n",
    "\n",
    "        :param query_template: Query template string\n",
    "        :param codigo: Error code\n",
    "        :param categoria: Error category\n",
    "        :param descricao: Error description\n",
    "        :return: Tuple (uninformed response, informed response)\n",
    "        \"\"\"\n",
    "        messages = [(\"human\", human_query), (\"system\", system_query)]\n",
    "\n",
    "        uninformed_query = self.comprehender.llm.invoke(messages)\n",
    "        informed_query = self.comprehender.query(self.namespace, self.index_name, human_query)\n",
    "\n",
    "        return {\n",
    "            \"uninformed\": uninformed_query,\n",
    "            \"informed\": informed_query\n",
    "        }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
